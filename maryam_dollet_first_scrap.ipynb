{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape One Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player(URL: str):\n",
    "    print(URL)\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(\"div\", class_=\"athlete_bio\")\n",
    "\n",
    "    if results != None:\n",
    "        table = results.find(\"table\", class_=\"biodata\")\n",
    "        player_bio = {}\n",
    "        player_bio[\"id\"] = URL[-1]\n",
    "        if results.find(\"img\", class_=\"photo\"):\n",
    "            player_bio[\"photo_link\"] = results.find(\"img\", class_=\"photo\")[\"src\"]\n",
    "        for player in table.findAll(\"tr\"):\n",
    "            # print(player.find(\"th\").text.replace(\" \", \"_\"))\n",
    "            # print(player.find(\"td\").text.lstrip())\n",
    "            player_bio[player.find(\"th\").text.replace(\" \", \"_\")] = player.find(\n",
    "                \"td\"\n",
    "            ).text.lstrip()\n",
    "            # print(i.find(\"td\").find(\"a\"))\n",
    "        games = soup.find(\"table\", class_=\"table\").findAll(\"tr\", class_=\"active\")\n",
    "        game_l = []\n",
    "        for game in games:\n",
    "            dict_cols = dict(\n",
    "                zip(\n",
    "                    [\"game\", \"discipline\", \"noc\"],\n",
    "                    [i.text.replace(\"\\n\", \"\") for i in game.findAll(\"td\")[:3]],\n",
    "                )\n",
    "            )\n",
    "            game_l.append(dict_cols)\n",
    "        # player_bio[\"games\"] = game_l\n",
    "\n",
    "        a = soup.find(\"table\", class_=\"table\").findAll(\"tr\")\n",
    "        a2 = a[1:]\n",
    "        # print(a[1:])\n",
    "        # print(len(a[1:]))\n",
    "        list_a = [b.has_attr(\"class\") for b in a[1:]]\n",
    "        print(list_a)\n",
    "        list_b = np.nonzero(list_a)[0]\n",
    "        print(list(itertools.pairwise(list_b)))\n",
    "        print(len(list(itertools.pairwise(list_b))) + 1)\n",
    "        list_c = []\n",
    "        for i in list(itertools.pairwise(list_b)):\n",
    "            # print(a2[i[0]:i[1]])\n",
    "            list_c.append(a2[i[0] : i[1]])\n",
    "        # print(a2[list_b[-1]:])\n",
    "        list_c.append(a2[list_b[-1] :])\n",
    "        len(list_c)\n",
    "        game_list = []\n",
    "        for game in list_c:\n",
    "            # print(game[0])\n",
    "            # print([i.text.replace(\"\\n\", \"\") for i in game[0].findAll(\"td\")[:3]])\n",
    "            dict_cols = dict(\n",
    "                zip(\n",
    "                    [\"game\", \"discipline\", \"noc\"],\n",
    "                    [i.text.replace(\"\\n\", \"\") for i in game[0].findAll(\"td\")[:3]],\n",
    "                )\n",
    "            )\n",
    "            # print(dict_cols)\n",
    "            events = []\n",
    "            for event in game[1:]:\n",
    "                # print([j.text.replace(\"\\n\", \"\") for j in event.findAll(\"td\")[1:-2]])\n",
    "                dict_event = dict(\n",
    "                    zip(\n",
    "                        [\"event\", \"team\", \"position\", \"medal\"],\n",
    "                        [j.text.replace(\"\\n\", \"\") for j in event.findAll(\"td\")[1:-2]],\n",
    "                    )\n",
    "                )\n",
    "                events.append(dict_event)\n",
    "            # print(events)\n",
    "            dict_cols[\"Events\"] = events\n",
    "            # print(dict_cols)\n",
    "            game_list.append(dict_cols)\n",
    "        # print(game_list)\n",
    "        player_bio[\"games\"] = game_list\n",
    "\n",
    "    return player_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_player(\"https://www.olympedia.org/athletes/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_player(\"https://www.olympedia.org/athletes/93860\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape All Olympic Games Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = []\n",
    "for i in range(1, 101):\n",
    "    URL = f\"https://www.olympedia.org/athletes/{i}\"\n",
    "    print(URL)\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(\"div\", class_=\"athlete_bio\")\n",
    "\n",
    "    if results != None:\n",
    "        table = results.find(\"table\", class_=\"biodata\")\n",
    "        player_bio = {}\n",
    "        player_bio[\"id\"] = i\n",
    "        if results.find(\"img\", class_=\"photo\"):\n",
    "            player_bio[\"photo_link\"] = results.find(\"img\", class_=\"photo\")[\"src\"]\n",
    "        for player in table.findAll(\"tr\"):\n",
    "            # print(player.find(\"th\").text.replace(\" \", \"_\"))\n",
    "            # print(player.find(\"td\").text.lstrip())\n",
    "            player_bio[player.find(\"th\").text.replace(\" \", \"_\")] = player.find(\n",
    "                \"td\"\n",
    "            ).text.lstrip()\n",
    "            # print(i.find(\"td\").find(\"a\"))\n",
    "        games = soup.find(\"table\", class_=\"table\").findAll(\"tr\", class_=\"active\")\n",
    "        game_l = []\n",
    "        for game in games:\n",
    "            dict_cols = dict(\n",
    "                zip(\n",
    "                    [\"game\", \"discipline\", \"noc\"],\n",
    "                    [i.text.replace(\"\\n\", \"\") for i in game.findAll(\"td\")[:3]],\n",
    "                )\n",
    "            )\n",
    "            game_l.append(dict_cols)\n",
    "        player_bio[\"games\"] = game_l\n",
    "\n",
    "        players.append(player_bio)\n",
    "\n",
    "with open(\"players.json\", \"w\") as f:\n",
    "    json.dump(players, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: There are about 150 000 players to scrape all this data we need to split it into threads but also send these threads to different computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape only players who won medals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need selenium to disable the limit of displayed players and to get all medal havers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medal_havers():\n",
    "    URL = \"https://www.olympedia.org/statistics/medal/athlete\"\n",
    "    page2 = requests.get(URL)\n",
    "\n",
    "    soup2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "    table = soup2.find(\"table\", class_=\"table\")\n",
    "\n",
    "    col_names = [i.text for i in table.find(\"thead\").findAll(\"th\")]\n",
    "    # print(col_names)\n",
    "\n",
    "    # create webdriver object\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(\"https://www.olympedia.org/statistics/medal/athlete\")\n",
    "\n",
    "    x = driver.find_element(By.NAME, \"limit\")\n",
    "    drop = Select(x)\n",
    "\n",
    "    # select by visible text\n",
    "    drop.select_by_visible_text(\"Show all\")\n",
    "    # wait for all the players to be displayed on the site\n",
    "    time.sleep(20)\n",
    "\n",
    "    players = driver.find_elements(By.CLASS_NAME, \"top\")\n",
    "    out = [i.get_attribute(\"innerHTML\") for i in players]\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    players_medal = []\n",
    "    for line in out:\n",
    "        soup = BeautifulSoup(line, \"html.parser\")\n",
    "        players_medal.append([i.text.lstrip() for i in soup.findAll(\"td\")])\n",
    "\n",
    "    df = pd.DataFrame(players_medal, columns=col_names)\n",
    "    df.to_csv(\"medal_havers.csv\", index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_medal_havers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Threading to get all the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_players(r: int):\n",
    "    logging.info(f\"Starting Thread in range {r} - {r + 100}\")\n",
    "    players = []\n",
    "    for i in range(r, r + 100):\n",
    "        URL = f\"https://www.olympedia.org/athletes/{str(i)}\"\n",
    "        # print(URL)\n",
    "        page = requests.get(URL)\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(\"div\", class_=\"athlete_bio\")\n",
    "\n",
    "        if results != None:\n",
    "            table = results.find(\"table\", class_=\"biodata\")\n",
    "            player_bio = {}\n",
    "            player_bio[\"id\"] = i\n",
    "            if results.find(\"img\", class_=\"photo\"):\n",
    "                player_bio[\"photo_link\"] = results.find(\"img\", class_=\"photo\")[\"src\"]\n",
    "            for player in table.findAll(\"tr\"):\n",
    "                player_bio[player.find(\"th\").text.replace(\" \", \"_\")] = player.find(\n",
    "                    \"td\"\n",
    "                ).text.lstrip()\n",
    "\n",
    "            games = soup.find(\"table\", class_=\"table\").findAll(\"tr\")\n",
    "            a2 = games[1:]\n",
    "            list_a = [b.has_attr(\"class\") for b in a2]\n",
    "            list_b = np.nonzero(list_a)[0]\n",
    "\n",
    "            list_c = []\n",
    "            for i in list(itertools.pairwise(list_b)):\n",
    "                list_c.append(a2[i[0] : i[1]])\n",
    "            list_c.append(a2[list_b[-1] :])\n",
    "\n",
    "            game_list = []\n",
    "            for game in list_c:\n",
    "                dict_cols = dict(\n",
    "                    zip(\n",
    "                        [\"game\", \"discipline\", \"noc\"],\n",
    "                        [i.text.replace(\"\\n\", \"\") for i in game[0].findAll(\"td\")[:3]],\n",
    "                    )\n",
    "                )\n",
    "                events = []\n",
    "                for event in game[1:]:\n",
    "                    dict_event = dict(\n",
    "                        zip(\n",
    "                            [\"event\", \"team\", \"position\", \"medal\"],\n",
    "                            [\n",
    "                                j.text.replace(\"\\n\", \"\")\n",
    "                                for j in event.findAll(\"td\")[1:-2]\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "                    events.append(dict_event)\n",
    "                dict_cols[\"Events\"] = events\n",
    "                game_list.append(dict_cols)\n",
    "            player_bio[\"games\"] = game_list\n",
    "\n",
    "            print(player_bio)\n",
    "\n",
    "            players.append(player_bio)\n",
    "    with open(f\"players/players_{r}.json\", \"w\") as f:\n",
    "        json.dump(players, f)\n",
    "    logging.info(f\"Thread finished {r} - {r + 100}\")\n",
    "    return players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside each batch there is 1000 players and there are 150 batches, which is a lot and must be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_func(batch):\n",
    "    thread_list = []\n",
    "    for i in batch:\n",
    "        thread = threading.Thread(target=scrape_players, args=(i,))\n",
    "        thread_list.append(thread)\n",
    "        thread.start()\n",
    "    for t in thread_list:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "for i in list(range(1, 150000, 1000)):\n",
    "    batches.append([i] + [y * 100 + i for y in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batches[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in batches:\n",
    "#     thread_func(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_files = [\"players_\" + str(y) + \".json\" for y in sorted([int(x.replace(\"players_\", \"\").replace(\".json\", \"\")) for x in os.listdir(\"players/\")])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_objects = []\n",
    "\n",
    "# Load each JSON file into a Python object.\n",
    "for json_file in sorted_files:\n",
    "    with open(f\"players/{json_file}\", \"r\") as f:\n",
    "        for player in json.load(f):\n",
    "            python_objects.append(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(python_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"all_players.json\", \"w\") as f:\n",
    "    json.dump(python_objects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_players.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.json_normalize(data, record_path=[\"games\"], meta=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "temp = pd.json_normalize(data[0], record_path=[\"games\"])\n",
    "# temp2 = pd.json_normalize(temp, record_path=[\"Events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = pd.concat([df.drop(['games'], axis = 1), temp], axis = 1)\n",
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[0])\n",
    "temp = pd.json_normalize(data[0], record_path=[\"games\"])\n",
    "Final = pd.concat([df.drop(['games'], axis = 1), temp], axis = 1)\n",
    "for player in data[1:]:\n",
    "    df2 = pd.DataFrame(player)\n",
    "    temp2 = pd.json_normalize(player, record_path=[\"games\"])\n",
    "    new_df = pd.concat([df2.drop(['games'], axis = 1), temp2], axis = 1)\n",
    "    Final = pd.concat([Final, new_df], ignore_index=True, sort=False)\n",
    "    print(len(Final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(Final.Events[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104060.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104060"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[:104060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[104060:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_part1.json\", \"w\") as f:\n",
    "    json.dump(data[:104060], f)\n",
    "with open(\"final_part2.json\", \"w\") as f:\n",
    "    json.dump(data[104060:], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yugioh-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
